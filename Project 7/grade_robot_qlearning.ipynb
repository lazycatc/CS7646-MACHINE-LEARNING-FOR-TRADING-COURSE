{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback as tb\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student modules to import\n",
    "main_code = \"QLearner\"  # module name to import\n",
    "\n",
    "robot_qlearning_testing_seed=1490652871\n",
    "random.seed(robot_qlearning_testing_seed)\n",
    "np.random.seed(robot_qlearning_testing_seed)\n",
    "QLearningTestCase = namedtuple('QLearning', ['description', 'group','world_file','best_reward','median_reward','max_time','points'])\n",
    "qlearning_test_cases = [\n",
    "    QLearningTestCase(\n",
    "        description=\"World 1\",\n",
    "        group='nodyna',\n",
    "        world_file='world01.csv',\n",
    "        best_reward=-17,\n",
    "        median_reward=-29.5,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 2\",\n",
    "        group='nodyna',\n",
    "        world_file='world02.csv',\n",
    "        best_reward=-14,\n",
    "        median_reward=-19,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 4\",\n",
    "        group='nodyna',\n",
    "        world_file='world04.csv',\n",
    "        best_reward=-24,\n",
    "        median_reward=-33,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 6\",\n",
    "        group='nodyna',\n",
    "        world_file='world06.csv',\n",
    "        best_reward=-16,\n",
    "        median_reward=-23.5,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 7\",\n",
    "        group='nodyna',\n",
    "        world_file='world07.csv',\n",
    "        best_reward=-14,\n",
    "        median_reward=-26,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 8\",\n",
    "        group='nodyna',\n",
    "        world_file='world08.csv',\n",
    "        best_reward=-14,\n",
    "        median_reward=-19,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 9\",\n",
    "        group='nodyna',\n",
    "        world_file='world09.csv',\n",
    "        best_reward=-15,\n",
    "        median_reward=-20,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 10\",\n",
    "        group='nodyna',\n",
    "        world_file='world10.csv',\n",
    "        best_reward=-28,\n",
    "        median_reward=-42,\n",
    "        max_time=2,\n",
    "        points=9.5\n",
    "    ),\n",
    "    # Dyna test cases\n",
    "    QLearningTestCase(\n",
    "        description=\"World 1, dyna=200\",\n",
    "        group='dyna',\n",
    "        world_file='world01.csv',\n",
    "        best_reward=-12,\n",
    "        median_reward=-29.5,\n",
    "        max_time=10,\n",
    "        points=2.5\n",
    "    ),\n",
    "    QLearningTestCase(\n",
    "        description=\"World 2, dyna=200\",\n",
    "        group='dyna',\n",
    "        world_file='world02.csv',\n",
    "        best_reward=-14,\n",
    "        median_reward=-19,\n",
    "        max_time=10,\n",
    "        points=2.5\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 5., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 5., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 2., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_file='world01.csv'\n",
    "world = np.array([list(map(float,s.strip().split(','))) for s in util.get_robot_world_file(world_file).readlines()])\n",
    "\n",
    "\n",
    "world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getrobotpos(data):\n",
    "    R = -999\n",
    "    C = -999\n",
    "    for row in range(0, data.shape[0]):\n",
    "        for col in range(0, data.shape[1]):\n",
    "            if data[row,col] == 2:\n",
    "                C = col\n",
    "                R = row\n",
    "    if (R+C)<0:\n",
    "        print (\"warning: start location not defined\")\n",
    "    return R, C\n",
    "\n",
    "# find where the goal is in the map\n",
    "def getgoalpos(data):\n",
    "    R = -999\n",
    "    C = -999\n",
    "    for row in range(0, data.shape[0]):\n",
    "        for col in range(0, data.shape[1]):\n",
    "            if data[row,col] == 3:\n",
    "                C = col\n",
    "                R = row\n",
    "    if (R+C)<0:\n",
    "        print (\"warning: goal location not defined\")\n",
    "    return (R, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the robot and report reward\n",
    "def movebot(data,oldpos,a):\n",
    "    testr, testc = oldpos\n",
    "\n",
    "    randomrate = 0.20 # how often do we move randomly\n",
    "    quicksandreward = -100 # penalty for stepping on quicksand\n",
    "\n",
    "    # decide if we're going to ignore the action and \n",
    "    # choose a random one instead\n",
    "    if random.uniform(0.0, 1.0) <= randomrate: # going rogue\n",
    "        a = random.randint(0,3) # choose the random direction\n",
    "\n",
    "    # update the test location\n",
    "    if a == 0: #north\n",
    "        testr = testr - 1\n",
    "    elif a == 1: #east\n",
    "        testc = testc + 1\n",
    "    elif a == 2: #south\n",
    "        testr = testr + 1\n",
    "    elif a == 3: #west\n",
    "        testc = testc - 1\n",
    "\n",
    "    reward = -1 # default reward is negative one\n",
    "    # see if it is legal. if not, revert\n",
    "    if testr < 0: # off the map\n",
    "        testr, testc = oldpos\n",
    "    elif testr >= data.shape[0]: # off the map\n",
    "        testr, testc = oldpos\n",
    "    elif testc < 0: # off the map\n",
    "        testr, testc = oldpos\n",
    "    elif testc >= data.shape[1]: # off the map\n",
    "        testr, testc = oldpos\n",
    "    elif data[testr, testc] == 1: # it is an obstacle\n",
    "        testr, testc = oldpos\n",
    "    elif data[testr, testc] == 5: # it is quicksand\n",
    "        reward = quicksandreward\n",
    "        data[testr, testc] = 6 # mark the event\n",
    "    elif data[testr, testc] == 6: # it is still quicksand\n",
    "        reward = quicksandreward\n",
    "        data[testr, testc] = 6 # mark the event\n",
    "    elif data[testr, testc] == 3:  # it is the goal\n",
    "        reward = 1 # for reaching the goal\n",
    "\n",
    "    return (testr, testc), reward #return the new, legal location\n",
    "\n",
    "# convert the location to a single integer\n",
    "def discretize(pos):\n",
    "    return pos[0]*10 + pos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_points = 100.0 \n",
    "html_pre_block = True  # surround comments with HTML <pre> tag (for T-Square comments field)\n",
    "\n",
    "# Test functon(s)\n",
    "@pytest.mark.parametrize(\"description,group,world_file,best_reward,median_reward,max_time,points\", qlearning_test_cases)\n",
    "def test_qlearning(description, group, world_file, best_reward, median_reward, max_time, points, grader):\n",
    "    points_earned = 0.0  # initialize points for this test case\n",
    "    try:\n",
    "        incorrect = True\n",
    "        if not 'QLearner' in globals():\n",
    "            import importlib\n",
    "            m = importlib.import_module('QLearner')\n",
    "            globals()['QLearner'] = m\n",
    "        # Unpack test case\n",
    "        world = np.array([list(map(float,s.strip().split(','))) for s in util.get_robot_world_file(world_file).readlines()])\n",
    "        student_reward = None\n",
    "        student_author = None\n",
    "        msgs = []\n",
    "        if group=='nodyna':\n",
    "            def timeoutwrapper_nodyna():\n",
    "                learner = QLearner.QLearner(num_states=100,\\\n",
    "                                            num_actions = 4, \\\n",
    "                                            alpha = 0.2, \\\n",
    "                                            gamma = 0.9, \\\n",
    "                                            rar = 0.98, \\\n",
    "                                            radr = 0.999, \\\n",
    "                                            dyna = 0, \\\n",
    "                                            verbose=False)\n",
    "                return qltest(worldmap=world,iterations=500,max_steps=10000,learner=learner,verbose=False)\n",
    "            student_reward = run_with_timeout(timeoutwrapper_nodyna,max_time,(),{})\n",
    "            incorrect = False\n",
    "            if student_reward < 1.5*median_reward:\n",
    "                incorrect = True\n",
    "                msgs.append(\"   Reward too low, expected %s, found %s\"%(median_reward,student_reward))\n",
    "        elif group=='dyna':\n",
    "            def timeoutwrapper_dyna():\n",
    "                learner = QLearner.QLearner(num_states=100,\\\n",
    "                                            num_actions = 4, \\\n",
    "                                            alpha = 0.2, \\\n",
    "                                            gamma = 0.9, \\\n",
    "                                            rar = 0.5, \\\n",
    "                                            radr = 0.99, \\\n",
    "                                            dyna = 200, \\\n",
    "                                            verbose=False)\n",
    "                return qltest(worldmap=world,iterations=50,max_steps=10000,learner=learner,verbose=False)\n",
    "            student_reward = run_with_timeout(timeoutwrapper_dyna,max_time,(),{})\n",
    "            incorrect = False\n",
    "            if student_reward < 1.5*median_reward:\n",
    "                incorrect = True\n",
    "                msgs.append(\"   Reward too low, expected %s, found %s\"%(median_reward,student_reward))\n",
    "        elif group=='author':\n",
    "            points_earned = -20\n",
    "            def timeoutwrapper_author():\n",
    "                learner = QLearner.QLearner(num_states=100,\\\n",
    "                                            num_actions = 4, \\\n",
    "                                            alpha = 0.2, \\\n",
    "                                            gamma = 0.9, \\\n",
    "                                            rar = 0.98, \\\n",
    "                                            radr = 0.999, \\\n",
    "                                            dyna = 0, \\\n",
    "                                            verbose=False)\n",
    "                return learner.author()\n",
    "            student_author = run_with_timeout(timeoutwrapper_author,max_time,(),{})\n",
    "            student_reward = best_reward+1\n",
    "            incorrect = False\n",
    "            if (student_author is None) or (student_author=='tb34'):\n",
    "                incorrect = True\n",
    "                msgs.append(\"   author() method not implemented correctly. Found {}\".format(student_author))\n",
    "            else:\n",
    "                points_earned = points\n",
    "        if (not incorrect):        \n",
    "            points_earned += points\n",
    "        if incorrect:\n",
    "            inputs_str = \"    group: {}\\n\" \\\n",
    "                         \"    world_file: {}\\n\"\\\n",
    "                         \"    median_reward: {}\\n\".format(group, world_file, median_reward)\n",
    "            raise (IncorrectOutput, \"Test failed on one or more output criteria.\\n  Inputs:\\n{}\\n  Failures:\\n{}\".format(inputs_str, \"\\n\".join(msgs)))\n",
    "    except Exception as e:\n",
    "        # Test result: failed\n",
    "        msg = \"Test case description: {}\\n\".format(description)\n",
    "        \n",
    "        # Generate a filtered stacktrace, only showing erroneous lines in student file(s)\n",
    "        tb_list = tb.extract_tb(sys.exc_info()[2])\n",
    "        for i in range(len(tb_list)):\n",
    "            row = tb_list[i]\n",
    "            tb_list[i] = (os.path.basename(row[0]), row[1], row[2], row[3])  # show only filename instead of long absolute path\n",
    "        tb_list = [row for row in tb_list if row[0] in ['QLearner.py','StrategyLearner.py']]\n",
    "        if tb_list:\n",
    "            msg += \"Traceback:\\n\"\n",
    "            msg += ''.join(tb.format_list(tb_list))  # contains newlines\n",
    "        elif 'grading_traceback' in dir(e):\n",
    "            msg += \"Traceback:\\n\"\n",
    "            msg += ''.join(tb.format_list(e.grading_traceback))\n",
    "        msg += \"{}: {}\".format(e.__class__.__name__, str(e))\n",
    "\n",
    "        # Report failure result to grader, with stacktrace\n",
    "        grader.add_result(GradeResult(outcome='failed', points=points_earned, msg=msg))\n",
    "        raise\n",
    "    else:\n",
    "        # Test result: passed (no exceptions)\n",
    "        grader.add_result(GradeResult(outcome='passed', points=points_earned, msg=None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def qltest(worldmap, iterations, max_steps, learner, verbose):\n",
    "# each iteration involves one trip to the goal\n",
    "    startpos = getrobotpos(worldmap) #find where the robot starts\n",
    "    goalpos = getgoalpos(worldmap) #find where the goal is\n",
    "    # max_reward = -float('inf')\n",
    "    all_rewards = list()\n",
    "    for iteration in range(1,iterations+1): \n",
    "        total_reward = 0\n",
    "        data = worldmap.copy()\n",
    "        robopos = startpos\n",
    "        state = discretize(robopos) #convert the location to a state\n",
    "        action = learner.query_set_state(state) #set the state and get first action\n",
    "        count = 0\n",
    "        while (robopos != goalpos) & (count<max_steps):\n",
    "\n",
    "            #move to new location according to action and then get a new action\n",
    "            newpos, stepreward = movebot(data,robopos,action)\n",
    "            if newpos == goalpos:\n",
    "                r = 1 # reward for reaching the goal\n",
    "            else:\n",
    "                r = stepreward # negative reward for not being at the goal\n",
    "            state = discretize(newpos)\n",
    "            action = learner.query(state,r)\n",
    "    \n",
    "            if data[robopos] != 6:\n",
    "                data[robopos] = 4 # mark where we've been for map printing\n",
    "            if data[newpos] != 6:\n",
    "                data[newpos] = 2 # move to new location\n",
    "            robopos = newpos # update the location\n",
    "            #if verbose: time.sleep(1)\n",
    "            total_reward += stepreward\n",
    "            count = count + 1\n",
    "        if verbose and (count == max_steps):\n",
    "            print (\"timeout\")\n",
    "        if verbose: printmap(data)\n",
    "        if verbose: print (iteration, total_reward)\n",
    "        # if max_reward < total_reward:\n",
    "        #     max_reward = total_reward\n",
    "        all_rewards.append(total_reward)\n",
    "    # return max_reward\n",
    "    return np.median(all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
